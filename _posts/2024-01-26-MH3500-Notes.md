---
layout: distill
title: NTU MH3500 Statistics
categories: Notes
date: 2024-01-26
description: Notes for NTU MH3500 Statistics.
tags:
  - Statistics
  - Probability Theory
giscus_comments: true
related_posts: false

authors:
  - name: Pu Fanyi
    url: "https://pufanyi.github.io"
    affiliations:
      name: NTU, Singapore
      url: "https://www.ntu.edu.sg/"

toc:
  - name: 什么是 statistics
  - name: Summarizing Data
  - name: From Normal Distribution
  - name: Limit Theorems
  - name: Parameter Estimation

bibliography: 2024-01-26-MH3500-Notes.bib
---

这学期学了门叫做 Statistics 的课，然后发现有点学不明白，所以记点笔记。

除了上课听到的，应该还会记一些书里看到的和自己想到的。

教材用的是这三本：<d-cite key="hogg2019introduction"></d-cite><d-cite key="rice2006mathematical"></d-cite><d-cite key="casella2021statistical"></d-cite>

2024 Spring 的课，lecturer 是 [Yeo Kwee Poo](https://www.linkedin.com/in/kwee-poo-yeo-ph-d-2966a43) 老师，tutor 是 [Mu Yue](https://www.linkedin.com/in/mu-yue-4a5a7a33/) 老师。

应该会想到啥写啥，所以可能比较混乱。另外因为里面有一大堆我自己乱七八糟的想法，所以里面的东西不一定对。

---

## 什么是 statistics

首先是 population, property, population distribution, random sample 这些名词，应该不用咋写。

Static inference 是指用 sample 来推断整个 population 的性质。

就是假设我们随机抽取了 $$n$$ 个 sample $$\boldsymbol{x} = (x_1, x_2, \dots, x_n)$$。我们假设这个 $$\boldsymbol{x}$$ 是由 $$\boldsymbol{X}=(X_1, X_2, \cdots, X_n)$$ 生成的。我们称 $$x_1, x_2, \dots, x_n$$ are the realizations of i.i.d. random variables $$X_1, X_2, \dots, X_n$$。也称 $$x_1, x_2, \dots, x_n$$ are observations of $$X_1, X_2, \dots, X_n$$。

而 statistic 事实上指的是 a real valued function, $$T(X_1, X_2, \dots, X_n)$$。需要注意的是，这个 function 之和 $$X_1, X_2, \dots, X_n$$ 有关，而不是 $$x_1, x_2, \dots, x_n$$。

然后 the distribution of a statistic is called a sampling distribution。也就是说，sampling distribution 是很多变量的 distribution，而 population 只是一个变量的 distribution。

Sample mean 是 $$\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i$$，sample variance 是 $$S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2$$。

{% details 为什么是 $$\frac{1}{n-1}$$ %}

感性理解是，你这 $$n$$ 个东西的平均值，事实上是根据这 $$n$$ 个东西的“趋向”有一定偏移的。就比如说我现在有一个 $$\mathcal{N}(0, 1)$$，我取两个 sample，假设拿到了 $$x_1=-1, x_2=2$$，这个时候他的 $\overline{X}$ 其实不是 $$0$$，而是 $$\frac{1}{2}$$，也就是被往右边拉过去了一点的。这时候如果我们用 $$\frac{1}{n}$$ 计算，那事实上算出来的值应该是偏小的（因为这里的 $$\overline{X}$$ 根据样本的抽取结果“调整”了一下）。而 $$\frac{1}{n-1}$$ 正好抵消了这一点。

下面是严格的数学证明。

我们有 $$\mathbb{E}(S^2)=\sigma^2$$。

我们考虑 $$\mathbb{E}(S^2)=\frac{1}{n-1}\sum_{i-1}^n\mathbb{E}\left[(X_i-\overline{X})^2\right]$$，于是我们考虑计算 $$\mathbb{E}\left[(X_i-\overline{X})^2\right]$$：

$$
\begin{aligned}
\mathbb{E}\left[\left(X_i-\overline{X}\right)^2\right] &= \mathbb{E}\left[\left((X_i-\mu)-(\overline{X}-\mu)\right)^2\right] \\
&= \mathbb{E}\left[(X_i-\mu)^2+(\overline{X}-\mu)^2-2(X_i-\mu)(\overline{X}-\mu)\right]\\
&=\mathbb{E}\left[(X_i-\mu)^2\right]+\mathbb{E}\left[(\overline{X}-\mu)^2\right]-2\mathbb{E}\left[(X_i-\mu)(\overline{X}-\mu)\right]\\
&=\mathrm{Var}(X_i)+\mathrm{Var}(\overline{X})-2\cdot\mathrm{Cov}(X_i,\overline{X})\\
&=\sigma^2+\mathrm{Var}\left(\frac{1}{n}\sum_{j=1}^nX_j\right)-2\cdot\mathrm{Cov}\left(X_i,\frac{1}{n}\sum_{j-1}^nX_j\right)\\
&=\sigma^2+\frac{1}{n^2}\sum_{j=1}^n\mathrm{Var}(X_j)-\frac{2}{n}\mathrm{Cov}(X_i,X_i)\\
&=\sigma^2+\frac{1}{n}\sigma^2-\frac{2}{n}\sigma^2\\
&=\frac{n-1}{n}\sigma^2
\end{aligned}
$$

于是我们有 $$\mathbb{E}(S^2)=\frac{1}{n-1}\sum_{i-1}^n\mathbb{E}\left[(X_i-\overline{X})^2\right]=\frac{1}{n-1}\sum_{i-1}^n\frac{n-1}{n}\sigma^2=\sigma^2$$。

{% enddetails %}

有一个好玩的性质是 如果 $$X_i\sim \mathcal{N}(\mu, \sigma^2)$$ 的话，$$\overline{X}$$ 和 $$S^2$$ 是独立的。上课老师讲了中方法但我有点没搞懂，Rice 书里有一个证法，感觉清楚一些。

{% details 上课老师讲的证法 %}

其实我至今没搞懂他到底是咋搞的。因为他用了一个结论，就是两个协方差为 $$0$$ 的 normal distribution 是相互独立的。不是很清楚不用 MGF 这玩意儿还能咋证（用 MGF 的话，那还不如 Rice 的证法呢）。

当然如果这玩意儿是成立的，那就好办了。就是毕竟我们是知道 $$\overline{X}\sim N\left(\mu, \frac{\sigma^2}{n}\right)$$，然后 $$X_i-\overline{X}\sim \mathcal{N}\left(0, \sigma^2\right)$$ 的。那我们只要搞出他俩的 covariance 就行了。

那我们算算：

$$
\begin{aligned}
\mathrm{Cov}\left(\overline{X}, X_i-\overline{X}\right)&=\mathrm{Cov}\left(\overline{X},X_i\right)-\mathrm{Cov}\left(\overline{X},\overline{X}\right)\\
&=\mathrm{Cov}\left(\frac{1}{n}\sum_{j=1}^nX_j,X_i\right)-\frac{\sigma^2}{n}\\
&=\frac{1}{n}\mathrm{Cov}\left(X_i, X_i\right)-\frac{\sigma^2}{n}\\
&=\frac{\sigma^2}{n}-\frac{\sigma^2}{n}\\
&=0
\end{aligned}
$$

然后他们就独立了。

{% enddetails %}

{% details Rice Book 的证法 %}

考虑到 $$S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2$$，我们其实只要证明 $$\overline{X}$$ 和 $$X_i-\overline{X}$$ 是独立的就可以了。

首先我们需要知道一件事情，就是假设有一个 $$r$$，使得：

$$
M_{\boldsymbol{X}}(\boldsymbol{t})=M_{\boldsymbol{X}}(t_1, t_2, \dots, t_r, 0, \dots, 0)\cdot M_{\boldsymbol{X}}(0, \dots, 0, t_{r+1}, t_{r+2}, t_n)
$$

那么 $$(t_1, t_2, \cdots, t_r)$$ 和 $$(t_{r+1}, t_{r+2}, \dots, t_n)$$ 是独立的。

这个证明是假设我们随机两次，第一次是 $$\boldsymbol{X}$$，第二次是 $$\boldsymbol{\widetilde{X}}$$。我们考虑：

$$
Y=(X_1, \dots, X_r, \widetilde{X}_{r+1}, \dots, \widetilde{X}_{n})
$$

由于 $$\boldsymbol{X}$$ 和 $$\boldsymbol{\widetilde{X}}$$ 是独立的，所以我们有：

$$
M_{\boldsymbol{Y}}(\boldsymbol{t})=M_{\boldsymbol{X}}(t_1, t_2, \dots, t_r, 0, \dots, 0)\cdot M_{\boldsymbol{\widetilde{X}}}(0, \dots, 0, t_{r+1}, t_{r+2}, t_n)
$$

而又因为 $$\boldsymbol{X}$$ 和 $$\boldsymbol{\widetilde{X}}$$ 其实是一样的，$$M_{\boldsymbol{X}}(\boldsymbol{t})=M_{\boldsymbol{\widetilde{X}}}(\boldsymbol{t})$$，也就是说：

$$
M_{\boldsymbol{Y}}(\boldsymbol{t})=M_{\boldsymbol{X}}(t_1, t_2, \dots, t_r, 0, \dots, 0)\cdot M_{\boldsymbol{X}}(0, \dots, 0, t_{r+1}, t_{r+2}, t_n)
$$

也就是 $$M_{\boldsymbol{X}}=M_{\boldsymbol{Y}}$$，也就是对于任意集合 $$A_1, A_2, \dots, A_n$$，我们有：

$$
\begin{aligned}
&\mathbb{P}\left\{(X_1\in A_1)\land (X_2\in A_2)\land\dots\land (X_n\in A_n)\right\}\\
=\ &\mathbb{P}\left\{(X_1\in A_1)\land \dots\land(X_r\in A_r)\land\left(\widetilde{X}_{r+1}\in A_{r+1}\right)\land\dots\land\left(\widetilde{X}_{n}\in A_n\right)\right\}\\
=\ &\mathbb{P}\left\{(X_1\in A_1)\land \dots\land(X_r\in A_r)\right\}\cdot\mathbb{P}\left\{\left(\widetilde{X}_{r+1}\in A_{r+1}\right)\land\dots\land\left(\widetilde{X}_{n}\in A_n\right)\right\}
\end{aligned}
$$

所以我们令 $$\boldsymbol{\widehat{X}}=\left(\overline X, X_1-\overline{X}, \dots, X_n-\overline{X}\right)$$，$$\boldsymbol{\widehat{t}}=(s, t_1, t_2,\dots, t_n)$$。

考虑到 $$X_1-\overline{X}, X_2-\overline{X},\dots,X_n-\overline{X}$$ 这些玩意儿的独立性是显然的，所以其实我们要证明的就是：

$$
M_{\boldsymbol{\widehat{X}}}\left(\boldsymbol{\widehat{t}}\right)=M_{\overline{X}}(s, 0, \dots, 0)\cdot M_{\boldsymbol{X}-\overline{X}}(0, t_1, \dots, t_n)
$$

然后我们接下来想做的是能不能把 $$M_{\widehat{\boldsymbol{X}}}(\widehat{\boldsymbol{t}})$$ 和 $$M_{\boldsymbol{X}}(\boldsymbol{a})$$ 联系起来。

为啥要这样转化呢？因为虽然我们现在不知道 $$\widehat{\boldsymbol{X}}$$ 的独立性，但是 $$\boldsymbol{X}$$ 的精神状态咋样我们是知道的。这样我们就能通过 $$\boldsymbol{X}$$ 把整个 MGF 给拆解开来，从而计算这个 MGF 到底是啥。

我们是这样做的：

$$
\begin{aligned}
M_{\boldsymbol{\widehat{X}}}\left(\boldsymbol{\widehat{t}}\right)&=\mathbb{E}\left[\exp\left({\widehat{\boldsymbol{X}}^\mathrm{T}\widehat{\boldsymbol{t}}}\right)\right]\\
&=\mathbb{E}\left[\exp\left({s\overline{X}+\sum_{i=1}^nt_i\left(X_i-\overline{X}\right)}\right)\right]\\
&=\mathbb{E}\left[\exp\left(\frac{s}{n}\sum_{i=1}^nX_i+\sum_{i=1}^nt_iX_i-\left(\sum_{i=1}^nt_i\right)\cdot\left(\frac{1}{n}\cdot\sum_{i=1}^nX_i\right)\right)\right]\\
&=\mathbb{E}\left[\exp\left(\sum_{i=1}^n\left(\frac{s}{n}+t_i-\overline{t}\right)X_i\right)\right]
\end{aligned}
$$

于是乎我们就可以令 $$a_i=\frac{s}{n}+t_i-\overline{t}$$，于是我们就有：

$$
M_{\boldsymbol{\widehat{X}}}\left(\boldsymbol{\widehat{t}}\right)=M_{\boldsymbol{X}}\left(\boldsymbol{a}\right)
$$

然后我们就能根据 $$\boldsymbol{X}$$ 的独立性把 $$M_{\boldsymbol{X}}\left(\boldsymbol{a}\right)$$ 拆开来了：

$$
\begin{aligned}
M_{\boldsymbol{\widehat{X}}}\left(\boldsymbol{\widehat{t}}\right)&=M_{\boldsymbol{X}}\left(\boldsymbol{a}\right)\\
&=\prod_{i=1}^nM_{X_i}\left(a_i\right)\\
&=\prod_{i = 1}^n\exp\left(\mu a_i+\frac{1}{2}\sigma^2a_i^2\right)\\
&=\exp\left(\mu\sum_{i=1}^na_i+\frac{\sigma^2}{2}\sum_{i=1}^na_i^2\right)
\end{aligned}
$$

考虑到：

$$
\begin{aligned}
\sum_{i=1}^na_i&=\sum_{i=1}^n\left(\frac{s}{n}+t_i-\overline{t}\right)\\
&=s+\sum_{i=1}^nt_i-n\overline{t}\\
&=s\\
\sum_{i=1}^na_i^2&=\sum_{i=1}^n\left(\frac{s}{n}+t_i-\overline{t}\right)^2\\
&=\sum_{i=1}^n\left(\left(\frac{s}{n}\right)^2+2\cdot\frac{s}{n}\left(t_i-\overline t\right)+\left(t_i-\overline{t}\right)^2\right)\\
&=\frac{s^2}{n}+\sum_{i=1}^n\left(t_i-\overline{t}\right)^2
\end{aligned}
$$

于是我们就有：

$$
\begin{aligned}
M_{\boldsymbol{\widehat{X}}}\left(\boldsymbol{\widehat{t}}\right)&=\exp\left(\mu\sum_{i=1}^na_i+\frac{\sigma^2}{2}\sum_{i=1}^na_i^2\right)\\
&=\exp\left(\mu s+\frac{\sigma^2}{2}\left(\frac{s^2}{n}+\sum_{i=1}^n\left(t_i-\overline{t}\right)^2\right)\right)\\
&=\exp\left(\mu s+\frac{\sigma^2}{2}\frac{s^2}{n}+\frac{\sigma^2}{2}\sum_{i=1}^n\left(t_i-\overline{t}\right)^2\right)\\
&=\exp\left(\mu s+\frac{1}{2}\cdot\frac{\sigma^2}{n}s^2\right)\cdot\prod_{i=1}^n\exp\left(\frac{\sigma^2}{2}\left(t_i-\overline{t}\right)^2\right)
\end{aligned}
$$

酱紫我们就已经把 $$M_{\boldsymbol{\widehat{X}}}\left(\boldsymbol{\widehat{t}}\right)$$ 给算出来了。简单算算就能发现

$$
M_{\boldsymbol{\widehat{X}}}\left(\boldsymbol{\widehat{t}}\right)=M_{\overline{X}}(s, 0, \dots, 0)\cdot M_{\boldsymbol{X}-\overline{X}}(0, t_1, \dots, t_n)
$$

这玩意儿是成立的。于是我们就证完了。

{% enddetails %}

接下来我们讨论的是数据的分类。对于一个 variable，根据以下的方式分个类：

![](/assets/svg/2024-01-26-MH3500-Notes/variable_class.svg)

---

## Summarizing Data

首先 Mean，Median，Mode 就不说了。

Skewness 说的是数据往哪儿偏，其实就是 standardize 之后的 3rd moment：

$$
\gamma_1=\frac{\mathbb{E}[(X-\mu)^3]}{\sigma^3}= \frac{\mathbb{E}[(X-\mu)^3]}{\mathbb{E}[(X-\mu)^2]^{3/2}}
$$

{% details 不同 skewness 的图像 %}

<div class="l-page">
  <iframe src="{{ '/assets/plotly/2024-01-26-MH3500-Notes/distributions/skewness.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>
{% enddetails %}

其中尾巴在左边的（$$\gamma_1<0$$）就叫 left-skewed，尾巴在右边的（$$\gamma_1>0$$）就叫 right-skewed。

{% details $$\log$$ and $$\exp$$ transformation %}
说一下我傻傻分不清的 $$\log$$ 和 $$\exp$$ transformation。

考虑到 $$\log$$ 函数是前面陡后面平的，他把左边那段的尾巴给拉长了。所以假如说 $$X\sim\mathcal{N}(\mu, \sigma^2)$$ 的话，那么 $$\log X$$ 他就是一个 left-skewed 的。同样的考虑到 $$\exp$$ 是后面比较陡，所以他把右边那一段给拉长了，于是 $$\exp X$$ 是 right-skewed 的。
{% enddetails %}

接着是 trimmed mean，就是我们比赛的时候看到的那种去掉最高分最低分的感觉。为啥要用 trimmed mean 捏？感性来讲的话，就是我们考虑到 mean 是很容易受到极端情况影响的，而 median 不会。但是 median 又没法反应整体情况。那办法就是去掉一些极端情况，因为其实 median 就是 trim 掉 $$0.5$$ 的 mean 嘛。

如果想更理性一点说的话。就是我们要考虑一个叫做 standard error 的东西。就是为啥咱有些时候不去 trim 捏？因为假设这个 distribution 是 normal distribution 的话，其实 trim 掉是比较亏的。就是我们考虑在做 normal distribution 的时候，所观测到 mean 和 median 的 standard error 是多少。考虑到 mean 其实就是 $$\mathcal{N}(\mu, \frac{\sigma^2}{n})$$，而 median 其实是 $$\mathcal{N}(\mu, \frac{\pi}{2}\cdot\frac{\sigma^2}{n})$$。所以其实我们是可以通过 mean 和 median 的 standard error 来判断我们是不是要 trim 的。

---

## From Normal Distribution

第一个需要讨论的是 Chi Square Distribution。就是我现在有 $$n$$ 个 i.i.d. $$\mathcal{N}(0, 1)$$ 的 random variables，$$X_1, X_2, \dots, X_n$$，我们定义 $$Z=\sum_{i=1}^nX_i^2 \sim \chi^2_n$$。

那就是我们咋计算这个 $$\chi^2_n$$ 呢，因为显然我们发现 $$\chi^2_n$$ 其实是 $$n$$ 个 $$\chi^2_1$$ 的和，所以我们第一件事就是计算 $$\chi^2_1$$。

这个好算，我们假设 $$Y\sim \mathcal{N}(0, 1), X=Y^2$$，那么 $$X\sim \chi_1^2$$：

$$
\begin{aligned}
F_X(x)&=\mathbb{P}(X\leq x)\\
&=\mathbb{P}(Y^2\leq x)\\
&=\mathbb{P}(-\sqrt{x}\leq Y\leq\sqrt{x})\\
&=\Phi(\sqrt{x})-\Phi(-\sqrt{x})\\
&=2\Phi(\sqrt{x})-1
\end{aligned}
$$

然后我们两边求个到得到 PDF：

$$
\begin{aligned}
f_X(x)&=\frac{\mathrm{d}F_X(x)}{\mathrm{d}x}\\
&=2\phi(\sqrt{x})\cdot\frac{1}{2\sqrt{x}}\\
&=\frac{1}{\sqrt{x}}\cdot\frac{1}{\sqrt{2\pi}}e^{-\frac{x}{2}}\\
&=\frac{1}{\sqrt{2\pi x}}e^{-\frac{x}{2}}
\end{aligned}
$$

然后就发现其实这玩意儿就是 $$\mathrm{Gamma}\left(\frac{1}{2}, 2\right)$$。

而考虑到 $$n$$ 个 $$\chi_1^2$$ 相加，那就是 $$n$$ 个 $$\mathrm{Gamma}\left(\frac{1}{2}, 2\right)$$ 相加，也就是 $$\mathrm{Gamma}\left(\frac{n}{2}, 2\right)$$。

{% details 不同自由度的 Chi Square Distribution %}

<div class="l-page">
  <iframe src="{{ '/assets/plotly/2024-01-26-MH3500-Notes/distributions/chi_square.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>
{% enddetails %}

{% details Gamma Distribution %}

写到这里的时候发现一些 Gamma Distribution 的细节已经给忘光光了。

首先需要复习的是 Gamma Distribution 的引入。他主要是想要推广 exponential distribution。就是我们想知道第 $$n$$ 次发生事情的时候，我们需要等多久。

我们是通过 Poisson Distribution 推下来的。就是我们考虑这个新 distribution 的 CDF $$F(t)$$，其实表示的是在 $$1\sim t$$ 这段时间里要发生大于等于 $$n$$ 次。于是我们有：

$$
F(t)=\sum_{k=n}^\infty\frac{(\lambda t)^ke^{-\lambda t}}{k!}
$$

那么我们就能轻松得到他的 PDF：

$$
\begin{aligned}
f(t)&=\frac{\mathrm{d}F(t)}{\mathrm{d}t}\\
&=\sum_{k=n}^{\infty}\frac{k\lambda^kt^{k-1}e^{-\lambda t}-\lambda^{k+1}t^ke^{-\lambda t}}{k!}\\
&=e^{-\lambda t}\left(\sum_{k=n}^\infty\frac{\lambda^k t^{k-1}}{(k-1)!}-\sum_{k=n}^\infty\frac{\lambda^{k+1}t^k}{k!}\right)\\
&=e^{-\lambda t}\left(\sum_{k=n-1}^{\infty}\frac{\lambda^{k+1}t^k}{k!}-\sum_{k=n}^\infty\frac{\lambda^{k+1}t^k}{k!}\right)\\
&=e^{-\lambda t}\cdot\frac{\lambda^nt^{n-1}}{(n-1)!}
\end{aligned}
$$

然后我们将 $$(n-1)!$$ 推广成 $$\Gamma(n)=\int_{0}^{+\infty} x^{n-1}e^{-x}\mathrm{d}x$$，就得到了 Gamma Distribution 的 PDF：

$$
f(t)=\frac{\lambda^n}{\Gamma(n)} t^{n-1}e^{-\lambda t}
$$

不过上课的时候老师比较习惯用 $$\Gamma(\alpha,\beta)$$ 来表示，也就是 $$\beta=\frac{1}{\lambda}$$，于是乎 PDF 就变成了：

$$
f(t)=\frac{t^{\alpha-1}e^{-\frac{t}{\beta}}}{\beta^\alpha\Gamma(\alpha)}
$$

一个有趣的性质就是 Gamma Distribution 的 $$n$$ 阶矩：

$$
\begin{aligned}
\mathbb{E}(X^n)&=\int_{0}^{+\infty}x^nf(x)\mathrm{d}x\\
&=\int_0^{+\infty}x^n\frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}\mathrm{d}x\\
&=\frac{\lambda^\alpha}{\Gamma(\alpha)}\int_0^{+\infty}x^{n+\alpha-1}\cdot\frac{1}{-\lambda}\cdot\mathrm{d}\left(e^{-\lambda x}\right)\\
&=-\frac{\lambda^{\alpha-1}}{\Gamma(\alpha)}\left(\left.x^{n+\alpha-1}e^{-\lambda x}\right|_0^{+\infty}-\int_0^{+\infty}e^{-\lambda x}\cdot\mathrm{d}\left(x^{n+\alpha-1}\right)\right)\\
&=\frac{\lambda^{\alpha-1}}{\Gamma(\alpha)}\int_0^{+\infty}e^{-\lambda x}\cdot(n+\alpha-1)x^{n+\alpha-2}\mathrm{d}x\\
&=\frac{n+\alpha-1}{\lambda}\int_{0}^{+\infty}x^{n-1}\frac{\lambda^\alpha}{\Gamma(\alpha)}x^{}e^{-\lambda x}\mathrm{d}x\\
&=\frac{n+\alpha-1}{\lambda}\mathbb{E}(X^{n-1})
\end{aligned}
$$

而 $$\mathbb{E}(X^{0})=1$$，所以

$$
\begin{aligned}
\mathbb{E}(X^n)&=\frac{1+\alpha-1}{\lambda}\cdot\frac{2+\alpha-1}{\lambda}\cdots\frac{n+\alpha-1}{\lambda}\\
&=\frac{\alpha^{\overline{n}}}{\lambda^n}
=\frac{\Gamma(n+\alpha)}{\Gamma(\alpha)\lambda^n}
\end{aligned}
$$

下面是一点小小的想法，就是这个 Gamma Function 是咋来的。其实就是考虑我们要把 $$n$$ 拓展到实数 $$\alpha$$，那么我们是必须要保证 $$\int_{0}^{+\infty}e^{-\lambda t}\cdot\frac{\lambda^\alpha t^{\alpha-1}}{\Gamma(\alpha)}=1$$。于是我们就可以得到：

$$
\Gamma(\alpha)=\int_{0}^{+\infty}e^{-\lambda t}\lambda^\alpha t^{\alpha-1}\mathrm{d}t
$$

而我们要的 $$\Gamma(\alpha)$$ 是想和 $$\lambda$$ 无关的，这时候我们考虑换元：

$$
\begin{aligned}
\Gamma(\alpha)&=\int_{0}^{+\infty}e^{-\lambda t}\lambda^\alpha t^{\alpha-1}\mathrm{d}t\\
&=\int_{0}^{+\infty}e^{-(\lambda t)}(\lambda t)^{\alpha-1}\mathrm{d}(\lambda t)\\
&=\int_{0}^{+\infty}e^{-x}x^{\alpha-1}\mathrm{d}x
\end{aligned}
$$

于是我们就得到了一个能够拟合 $$(n-1)!$$ 的 Gamma 函数。

下面是不同 $$\alpha$$ 和 $$\beta$$ 的 Gamma Distribution 的图像：

<div class="l-page">
  <iframe src="{{ '/assets/plotly/2024-01-26-MH3500-Notes/distributions/gamma_distribution_change_alpha.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

<div class="l-page">
  <iframe src="{{ '/assets/plotly/2024-01-26-MH3500-Notes/distributions/gamma_distribution_change_beta.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

{% enddetails %}

接着我们要讨论的是当 $$X_1, X_2, \dots, X_n\sim \mathcal{N}(\mu, \sigma^2)$$ 时，$$S^2$$ 的分布和 Chi-Square Distribution 的关系：

$$
\frac{(n-1)S^2}{\sigma^2}\sim \chi_{n-1}^2
$$

{% details 证明 %}
首先我们需要观察到的是：

$$
\sum_{i=1}^n\left(\frac{X_i-\mu}{\sigma}\right)^2\sim\chi^2_n
$$

接着我们考虑通过 $$\left(X_i-\mu\right)^2$$ 来得到 $$\left(X_i-\overline{X}\right)^2$$：

$$
\begin{aligned}
\sum_{i=1}^n\left(\frac{X_i-\mu}{\sigma}\right)^2&=\frac{1}{\sigma^2}\sum_{i=1}^n\left(\left(X_i-\overline{X}\right)+\left(\overline{X}-\mu\right)\right)^2\\
&=\frac{1}{\sigma^2}\sum_{i=1}^n\left(X_i-\overline{X}\right)^2+\frac{2}{\sigma^2}\sum_{i=1}^n\left(X_i-\overline{X}\right)\left(\overline{X}-\mu\right)+\frac{1}{\sigma^2}\sum_{i=1}^n\left(\overline{X}-\mu\right)^2\\
&=\frac{1}{\sigma^2}\sum_{i=1}^n\left(X_i-\overline{X}\right)^2+\frac{2}{\sigma^2}\left(\overline{X}-\mu\right)\sum_{i=1}^n\left(X_i-\overline{X}\right)+\frac{n}{\sigma^2}\left(\overline{X}-\mu\right)^2\\
&=\frac{1}{\sigma^2}\sum_{i=1}^n\left(X_i-\overline{X}\right)^2+\left(\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\right)^2
\end{aligned}
$$

然后我们发现其实他是一个 $$W=U+V$$ 的形式，其中 $$W=\sum_{i=1}^n\left(\frac{X_i-\mu}{\sigma}\right)^2\sim \chi_n^2$$，$$V=\left(\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\right)^2\sim \chi_1^2$$，我们唯一不知道也是想要知道的是 $$U=\frac{1}{\sigma^2}\sum_{i=1}^n\left(X_i-\overline{X}\right)^2=\frac{(n-1)S^2}{\sigma^2}$$。

由于 $$X_i-\overline{X}$$ 和 $$\overline{X}$$ 是独立的，于是 $$U$$ 和 $$V$$ 也是独立的。于是我们有：

$$
M_W(t)=M_U(t)\cdot M_V(t)
$$

那我们很容易解出：

$$
\begin{aligned}
M_U(t)&=\frac{M_W(t)}{M_v(t)}\\
&=\frac{(1-2t)^{-\frac{n}{2}}}{(1-2t)^{-\frac{1}{2}}}\\
&=(1-2t)^{-\frac{n-1}{2}}
\end{aligned}
$$

也就是

$$
\frac{(n-1)S^2}{\sigma^2}\sim \chi_{n-1}^2
$$

于是就证完了。
{% enddetails %}

接下来是 $$t$$ distribution。Motivation 是这样的，我们现在还是有 $$n$$ 个 i.i.d. $$\mathcal{N}(\mu, \sigma^2)$$ 的 random variables，$$X_1, X_2, \dots, X_n$$。我们知道 $$\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\sim \mathcal{N}(0, 1)$$，我们现在想知道的是 $$Z=\frac{\overline{X}-\mu}{S/\sqrt{n}}$$ 的 distribution 长啥样。

我们考虑：

$$
Z=\frac{\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}}{\sqrt{\frac{(n-1)S/\sigma^2}{n-1}}}=\frac{C}{\sqrt{D/(n-1)}}
$$

我们发现 $$C=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\sim \mathcal{N}(0, 1)$$，$$D=\frac{(n-1)S}{\sigma^2}\sim \chi_{n-1}^2$$，而且 $$C$$ 和 $$D$$ 是独立的。

于是我们就这样考虑定义一个新的 distribution：

$$
t_k=\frac{C}{\sqrt{D_k/k}}
$$

其中 $$C\sim \mathcal{N}(0, 1)$$，$$D_k\sim \chi_k^2$$ 且 $$C$$ 和 $$D_k$$ 是独立的。

于是我们有：

$$
\frac{\overline{X}-\mu}{S/\sqrt{n}}\sim t_{n-1}
$$

而我们知道在 $$n$$ 越来越大的时候，$$S$$ 是越来越趋向于 $$\sigma$$ 的，也就是说当 $$n$$ 变大的时候，$$t_{n}$$ 会越来越像 $$\mathcal{N}(0, 1)$$。

{% details 不同自由度的 $$t$$ Distribution %}

<div class="l-page">
  <iframe src="{{ '/assets/plotly/2024-01-26-MH3500-Notes/distributions/t_distribution.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>
{% enddetails %}

接下来是 $$t_n$$ 的表达式，我们有：

$$
f_T(t)=\frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi}\cdot\Gamma\left(\frac{n}{2}\right)}\left(1+\frac{t^2}{n}\right)^{-\frac{n+1}{2}}
$$

{% details 证明 %}

首先一个需要解决的小问题就是我们需要知道俩随机变量是咋除的。

我们考虑现在有俩独立的随机变量 $$X, Y$$，然后我们考虑 $$Z = \frac{X}{Y}$$。

我们考虑：

$$
\begin{aligned}
F_Z(z)&=\mathbb{P}(Z\leq z)\\
&=\mathbb{P}\left(\frac{X}{Y}\leq z\right)\\
&=\mathbb{P}\left(X\geq zY\land Y<0\right)+\mathbb{P}\left(X\leq zY\land Y>0\right)\\
&=\int_{-\infty}^{0}\mathbb{P}(X\ge zy)f_Y(y)\mathrm{d}y+\int_{0}^{+\infty}\mathbb{P}(X\le zy)f_Y(y)\mathrm{d}y\\
&=\int_{-\infty}^0\left(\int_{zy}^{+\infty}f_X(x)\mathrm{d}x\right)f_{Y}(y)\mathrm{d}y+\int_{0}^{+\infty}\left(\int_{-\infty}^{zy}f_X(x)\mathrm{d}x\right)f_Y(y)\mathrm{d}y
\end{aligned}
$$

于是我们可以通过两边求导得到 $$f_Z(z)$$：

$$
\begin{aligned}
f_Z(z)&=\frac{\mathrm{d}F_Z(z)}{\mathrm{d}z}\\
&=\int_{-\infty}^0\left(-y\cdot f_{X}(zy)\right)\cdot f_Y(y)\mathrm{d}y+\int_{0}^{+\infty}\left(y\cdot f_{X}(zy)\right)\cdot f_Y(y)\mathrm{d}y\\
&=\int_{-\infty}^{+\infty}|y|\cdot f_X(zy)\cdot f_Y(y)\mathrm{d}y
\end{aligned}
$$

于是乎我们就能通过这玩意儿来说明 $$t_k$$ 是啥。

当然在此之前我们先把 $$\sqrt{D_n/n}$$ 的 PDF 给求出来。

我们考虑到：

$$
\begin{aligned}
F_{\sqrt{D_n/n}}(t)&=\mathbb{P}\left(\sqrt{\frac{D_n}{n}}\leq t\right)\\
&=\mathbb{P}\left(-nt^2\le D_n\leq nt^2\right)\\
&=\int_{-nt^2}^{nt^2}f_{D_n}(x)\mathrm{d}x\\
&=\int_{0}^{nt^2}\frac{x^{\frac{n}{2}-1}e^{-\frac{x}{2}}}{2^{\frac{n}{2}}\Gamma\left(\frac{n}{2}\right)}\mathrm{d}x\\
&=\frac{1}{2^{\frac{n}{2}}\Gamma\left(\frac{n}{2}\right)}\int_{0}^{nt^2}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}\mathrm{d}x
\end{aligned}
$$

然后我们对两边求个导：

$$
\begin{aligned}
f_{\sqrt{D_n/n}}(t)&=\frac{\mathrm{d}F_{\sqrt{D_n/n}}(t)}{\mathrm{d}t}\\
&=\frac{1}{2^{\frac{n}{2}}\Gamma\left(\frac{n}{2}\right)}\cdot 2nt\cdot (nt^2)^{\frac{n}{2}-1}e^{-\frac{nt^2}{2}}\\
&=\frac{n^{\frac{n}{2}}{t}^{n-1}e^{-\frac{nt^2}{2}}}{2^{\frac{n}{2}-1}\Gamma\left(\frac{n}{2}\right)}
\end{aligned}
$$

当然是 $$t\ge 0$$ 的时候，当 $$t<0$$ 的时候就是 $$0$$ 了。

然后我们带入公式得到 $$f_{T}(t)$$：

$$
\begin{aligned}
f_T(t)&=\int_{-\infty}^{+\infty}|y|\cdot f_{C}(ty)\cdot f_{\sqrt{D_k/k}}(y)\mathrm{d}y\\
&=\int_{0}^{+\infty}y\cdot\frac{1}{\sqrt{2\pi}}e^{-\frac{(ty)^2}{2}}\cdot\frac{k^{\frac{k}{2}}{y}^{k-1}e^{-\frac{ky^2}{2}}}{2^{\frac{k}{2}-1}\Gamma\left(\frac{k}{2}\right)}\mathrm{d}y\\
&=\frac{k^{\frac{k}{2}}}{\sqrt{2\pi}\cdot 2^{\frac{k}{2}-1}\cdot \Gamma\left(\frac{k}{2}\right)}\int_{0}^{+\infty}y^ke^{-\frac{(k+t^2)y^2}{2}}\mathrm{d}y
\end{aligned}
$$

我们发现这个后面那个积分式子特别像 $$\Gamma$$ 函数，于是我们来凑一凑。考虑到 $$\Gamma$$ 函数中 $$e$$ 那边是个 $$e^{-t}$$ 的形式，那么就换个元。为了方便书写我们假设 $$\alpha=\frac{k+t^2}{2}$$，$$u=\alpha y^2$$。

于是乎我们就有：

$$
\begin{aligned}
\int_{0}^{+\infty}y^ke^{-\frac{(k+t^2)y^2}{2}}\mathrm{d}y&=\int_{0}^{+\infty}\left(\frac{u}{\alpha}\right)^\frac{k}{2}e^{-u}\cdot \mathrm{d}\left(\sqrt{\frac{u}{\alpha}}\right)\\
&=\int_0^{+\infty}\left(\frac{u}{\alpha}\right)^{\frac{k}{2}}e^{-u}\cdot \frac{1}{\alpha}\cdot\frac{1}{2}\cdot\sqrt{\frac{\alpha}{u}}\cdot\mathrm{d}u\\
&=\frac{1}{2\alpha}\int_0^{+\infty}\left(\frac{u}{\alpha}\right)^\frac{k-1}{2}e^{-u}\mathrm{d}u\\
&=\frac{1}{2\alpha^{\frac{k+1}{2}}}\int_0^{+\infty}u^{\frac{k-1}{2}}e^{-u}\mathrm{d}u\\
&=\frac{1}{2\alpha^{\frac{k+1}{2}}}\int_0^{+\infty}u^{\left(\frac{k+1}{2}\right)-1}e^{-u}\mathrm{d}u\\
&=\frac{1}{2\alpha^{\frac{k+1}{2}}}\cdot\Gamma\left(\frac{k+1}{2}\right)\\
\end{aligned}
$$

然后我们最后把所有东西一股脑代进去大概整理一下得到答案：

$$
\begin{aligned}
f_T(t)&=\frac{k^{\frac{k}{2}}}{\sqrt{2\pi}\cdot 2^{\frac{k}{2}-1}\cdot \Gamma\left(\frac{k}{2}\right)}\int_{0}^{+\infty}y^ke^{-\frac{(k+t^2)y^2}{2}}\mathrm{d}y\\
&=\frac{k^{\frac{k}{2}}}{\sqrt{2\pi}\cdot 2^{\frac{k}{2}-1}\cdot\Gamma\left(\frac{k}{2}\right)}\cdot\frac{1}{2\alpha^{\frac{k+1}{2}}}\cdot\Gamma\left(\frac{k+1}{2}\right)\\
&=\frac{\Gamma\left(\frac{k+1}{2}\right)\cdot 2^{\frac{k+1}{2}}\cdot k^{\frac{k}{2}}}{\sqrt{\pi}\cdot\Gamma\left(\frac{k}{2}\right)\cdot 2^{\frac{k+1}{2}}\cdot\left(k+t^2\right)^{\frac{k+1}{2}}}\\
&=\frac{1}{\sqrt{\pi}}\cdot\frac{\Gamma\left(\frac{k+1}{2}\right)}{\Gamma\left(\frac{k}{2}\right)}\cdot\frac{k^{\frac{k}{2}}}{\left(k+t^2\right)^{\frac{k+1}{2}}}\\
&=\frac{1}{\sqrt{\pi}}\cdot\frac{\Gamma\left(\frac{k+1}{2}\right)}{\Gamma\left(\frac{k}{2}\right)}\cdot\frac{1}{\sqrt{k}}\cdot\left(\frac{k}{k+t^2}\right)^{\frac{k+1}{2}}\\
&=\frac{\Gamma\left(\frac{k+1}{2}\right)}{\sqrt{k\pi}\cdot\Gamma\left(\frac{k}{2}\right)}\cdot\left(1+\frac{t^2}{k}\right)^{-\frac{k+1}{2}}
\end{aligned}
$$

于是我们就得到了最终的 PDF：

$$
f_T(t)=\frac{\Gamma\left(\frac{k+1}{2}\right)}{\sqrt{k\pi}\cdot\Gamma\left(\frac{k}{2}\right)}\cdot\left(1+\frac{t^2}{k}\right)^{-\frac{k+1}{2}}
$$

{% enddetails %}

接下来是 $$F$$ distribution。Motivation 是我们有俩 i.i.d. 的 random variables $$U_m\sim \chi_m^2, V_n\sim \chi_n^2$$，我们想知道 $$F=\frac{U_m/m}{V_n/n}$$ 的 distribution 长啥样。

PDF 是这样的：

$$
f(w)=\frac{\Gamma\left(\frac{n+m}{2}\right)}{\Gamma\left(\frac{m}{2}\right)\Gamma\left(\frac{n}{2}\right)}\cdot\left(\frac{m}{n}\right)^{\frac{m}{2}}\cdot w^{\frac{m}{2}-1}\cdot\left(1+\frac{m}{n}w\right)^{-\frac{m+n}{2}}
$$

{% details 证明 %}

首先我们先要知道 $$U_m/m$$ 长啥样。

$$
\begin{aligned}
F_{U_m/m}(t)&=\mathbb{P}\left(\frac{U_m}{m}\leq t\right)\\
&=\mathbb{P}\left(U_m\leq mt\right)\\
&=F_{U_m}(mt)\\
\end{aligned}
$$

然后两边求导：

$$
\begin{aligned}
f_{U_m/m}(t)&=\frac{\mathrm{d}F_{U_m/m}(t)}{\mathrm{d}t}\\
&=f_{U_m}(mt)\cdot m\\
&=\frac{m\cdot (mt)^{\frac{m}{2}-1}\cdot e^{-\frac{mt}{2}}}{2^{\frac{m}{2}}\cdot \Gamma\left(\frac{m}{2}\right)}\\
&=\frac{m^{\frac{m}{2}}\cdot t^{\frac{m}{2}-1}\cdot e^{-\frac{mt}{2}}}{2^{\frac{m}{2}}\cdot \Gamma\left(\frac{m}{2}\right)}\\
\end{aligned}
$$

于是我们也有：

$$
f_{V_n/n}(t)=\frac{n^{\frac{n}{2}}\cdot t^{\frac{n}{2}-1}\cdot e^{-\frac{nt}{2}}}{2^{\frac{n}{2}}\cdot \Gamma\left(\frac{n}{2}\right)}
$$

于是我们带入之前两个变量除法的柿子：

$$
\begin{aligned}
f_F(w)&=\int_{-\infty}^{+\infty}|t|\cdot f_{U_m/m}(tw)\cdot f_{V_n/n}(t)\mathrm{d}t\\
&=\int_{0}^{+\infty}t\cdot f_{U_m/m}(tw)\cdot f_{V_n/n}(t)\mathrm{d}t\\
&=\int_0^{+\infty}t\cdot\frac{m^{\frac{m}{2}}\cdot (tw)^{\frac{m}{2}-1}\cdot e^{-\frac{mtw}{2}}}{2^{\frac{m}{2}}\cdot \Gamma\left(\frac{m}{2}\right)}\cdot\frac{n^{\frac{n}{2}}\cdot t^{\frac{n}{2}-1}\cdot e^{-\frac{nt}{2}}}{2^{\frac{n}{2}}\cdot \Gamma\left(\frac{n}{2}\right)}\mathrm{d}t\\
&=\frac{m^\frac{m}{2}n^\frac{n}{2}\cdot w^{\frac{w}{2}-1}}{2^{\frac{m+n}{2}}\cdot \Gamma\left(\frac{m}{2}\right)\cdot \Gamma\left(\frac{n}{2}\right)}\int_0^{+\infty}t^{\frac{m+n}{2}-1}\cdot e^{-\frac{(mw+n)\cdot t}{2}}\mathrm{d}t
\end{aligned}
$$

故技重施换个元：

$$
u=\frac{(mw+n)}{2}t
$$

于是我们整理一下就可以了！

$$
\begin{aligned}
f_F(w)&=\frac{m^\frac{m}{2}n^\frac{n}{2}\cdot w^{\frac{w}{2}-1}}{2^{\frac{m+n}{2}}\cdot \Gamma\left(\frac{m}{2}\right)\cdot \Gamma\left(\frac{n}{2}\right)}\int_0^{+\infty}t^{\frac{m+n}{2}-1}\cdot e^{-\frac{(mw+n)\cdot t}{2}}\mathrm{d}t\\
&=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}\cdot w^{\frac{m}{2}-1}}{2^{\frac{m+n}{2}}\Gamma\left(\frac{m}{2}\right)\Gamma\left(\frac{n}{2}\right)}\int_{0}^{+\infty}\left(\frac{2u}{mw+n}\right)^{\frac{m + n}{2} - 1}e^{-u}\cdot\frac{2}{mw+n}\mathrm{d}u\\
&=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}\cdot w^{\frac{m}{2}-1}}{(mw+n)^{\frac{m + n}{2}}\Gamma\left(\frac{m}{2}\right)\Gamma\left(\frac{n}{2}\right)}\int_{0}^{+\infty}u^{\frac{m+n}{2}-1}e^{-u}\mathrm{d}u\\
&=\frac{\Gamma\left(\frac{m+n}{2}\right)}{\Gamma\left(\frac{m}{2}\right)\Gamma\left(\frac{n}{2}\right)}\cdot\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}\cdot w^{\frac{m}{2}-1}}{(mw+n)^{\frac{m + n}{2}}}\\
&=\frac{\Gamma\left(\frac{m+n}{2}\right)}{\Gamma\left(\frac{m}{2}\right)\Gamma\left(\frac{n}{2}\right)}\cdot w^{\frac{m}{2}-1}\cdot\left(\frac{m}{n}\right)^{\frac{m}{2}}\cdot n^{\frac{m}{2}}\cdot n^{\frac{n}{2}}\cdot\frac{1}{(mw+n)^{\frac{m + n}{2}}}\\
&=\frac{\Gamma\left(\frac{m+n}{2}\right)}{\Gamma\left(\frac{m}{2}\right)\Gamma\left(\frac{n}{2}\right)}\cdot\left(\frac{m}{n}\right)^{\frac{m}{2}}\cdot w^{\frac{m}{2}-1}\cdot\left(\frac{n}{mw+n}\right)^{\frac{m + n}{2}}\\
&=\frac{\Gamma\left(\frac{m+n}{2}\right)}{\Gamma\left(\frac{m}{2}\right)\Gamma\left(\frac{n}{2}\right)}\cdot\left(\frac{m}{n}\right)^{\frac{m}{2}}\cdot w^{\frac{m}{2}-1}\cdot\left(1+\frac{m}{n}w\right)^{-\frac{m + n}{2}}
\end{aligned}
$$

当然很多教材上我们会引入一个很像组合数的 Beta 函数：

$$
B(z_1, z_2)=\frac{\Gamma(z_1)\Gamma(z_2)}{\Gamma(z_1+z_2)}
$$

然后就能简写上面的式子。

{% enddetails %}

{% details 不同自由度的 $$F$$ Distribution %}

<div class="l-page">
  <iframe src="{{ '/assets/plotly/2024-01-26-MH3500-Notes/distributions/F_distribution_change_M.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>
<div class="l-page">
  <iframe src="{{ '/assets/plotly/2024-01-26-MH3500-Notes/distributions/F_distribution_change_N.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

{% enddetails %}

关于 $$t$$ distribution 和 $$F$$ distribution 有一定联系：

$$
t_n^2\sim \frac{\mathcal{N}(0, 1)^2}{\chi_n^2/n}\sim \frac{\chi_1^2/1}{\chi_n^2/n}\sim F(1, n)
$$

---

## Limit Theorems

这玩意儿其实我学概率的时候就没咋学好。

就是首先是俩不等式。

第一个是 Markov Inequality，就是假设有一个非负的随机变量 $$X$$，我们可以预测其尾部事件发生的概率上界：

$$
\mathbb{P}(X\ge c)\le\frac{\mathbb{E}(X)}{c}
$$

当然也有教材（比如 Hogg 那本 <d-cite key="hogg2019introduction"></d-cite>）说的是随便一个随机变量 $$X$$，然后如果 $$u(X)$$ 为非负函数，那么：

$$
\mathbb{P}(u(X)\ge c)\le\frac{\mathbb{E}(u(X))}{c}
$$

不过反正都是一样的其实。

一个简单的理解方式就是，“不超过 $$\frac{1}{n}$$ 的人拥有超过平均 $$n$$ 倍的工资”。

{% details 证明 %}

其实证明是很简都嘟，我们首先考虑“不超过 $$\frac{1}{n}$$ 的人拥有超过平均 $$n$$ 倍的工资”这个命题。因为如果超过 $$\frac{1}{n}$$ 了，那么就算其他人啥都没有（也就是 $$0$$），那平均也已经超过 $$\frac{1}{n}\times n\times \mathbb{E}(x)$$ 了，那显然是だめ的。

那我们照猫画虎证明一下尊命题。就是因为我们考虑到 $$X>0$$ 所以说 $$x\cdot f(x)>0$$。那么很显然的

$$
\begin{aligned}
\mathbb{E}(X)&=\int_0^\infty x\cdot f(x)\mathrm{d}x\\
&\ge \int_{c}^\infty x\cdot f(x)\mathrm{d}x\\
&\ge c\int_{c}^\infty f(x)\mathrm{d}x\\
&=c\cdot \mathbb{P}(X\ge c)
\end{aligned}
$$

{% enddetails %}

第二个是 Chebyshev's Inequality。就是假设 $$\mathbb{E}(X)$$ 和 $$\mathrm{Var}(X)$$ 都存在的情况下，我们考虑变量离均值的距离：

$$
\mathbb{P}(|X - \mathbb{E}(X)|\ge t)\le \frac{\mathrm{Var}(X)}{t^2}
$$

{% details 证明 %}

这个其实就是一个 Markov Inequality 的拓展，我们考虑：

$$
\begin{aligned}
\mathbb{P}(|X - \mathbb{E}(X)|\ge t)&=\mathbb{P}((X - \mathbb{E}(X))^2\ge t^2)\\
&\le \frac{\mathbb{E}((X - \mathbb{E}(X))^2)}{t^2}\\
&=\frac{\mathrm{Var}(X)}{t^2}
\end{aligned}
$$

{% enddetails %}

然后是跟朋友讨论的时候看到的一个有趣的不等式，是一个很像 Chebyshev's Inequality 的东西，叫做 Cantelli's Inequality。其实就是单边的 Chebyshev's Inequality：

$$
\mathbb{P}(X - \mathbb{E}(X)\ge t)\le \frac{\mathrm{Var}(X)}{\mathrm{Var}(X) + t^2}
$$

{% details 证明 %}

当时是他们期中考的一道考题，然鹅我想了半天没想出来。

我们考虑的是把整个变量平移之后再平方，使得左边那部分可以忽略。假设我们平移了 $$a$$：

$$
\begin{aligned}
\mathbb{P}(X - \mathbb{E}(X)\ge t)&=\mathbb{P}((X) - \mathbb{E}(X) + a\ge t + a)\\
&\le \mathbb{P}\left(\left|X - \mathbb{E}(X) + a\right|\ge t + a\right)\\
&=\mathbb{P}\left(\left(X - \mathbb{E}(X) + a\right)^2\ge \left(t + a\right)^2\right)\\
&\le \frac{\mathbb{E}\left(\left(X - \mathbb{E}(X) + a\right)^2\right)}{\left(t + a\right)^2}\\
&=\frac{\mathbb{E}\left(\left(X - \mathbb{E}(X)\right)^2+2\cdot a\cdot\left(X - \mathbb{E}(X)\right)+a^2\right)}{\left(t + a\right)^2}\\
&=\frac{\mathbb{E}\left((X - \mathbb{E}(X))^2\right) + a^2}{\left(t + a\right)^2}\\
&=\frac{\mathrm{Var}(X) + a^2}{\left(t + a\right)^2}
\end{aligned}
$$

然后我们因为是要求一个下界，也就是这个 $$a$$ 是可以随便取的。那我们就求个导，看看取什么 $$a$$ 最好：

$$
\begin{aligned}
\frac{\partial}{\partial a}\left(\frac{\mathrm{Var}(X) + a^2}{\left(t + a\right)^2}\right)&=\frac{2a(t+a)^2-2(t+a)\cdot\left(a^2+\mathrm{Var}(X)\right)}{(t+a)^4}\\
&=\frac{2at-2\cdot\mathrm{Var}(X)}{(t+a)^3}
\end{aligned}
$$

现在我们要让这个东西等于 $$0$$，那么我们就有：

$$
\frac{2at-2\cdot\mathrm{Var}(X)}{(t+a)^3}=0
$$

也就是：

$$
a=\frac{\mathrm{Var}(X)}{t}
$$

那我们把这个 $$a$$ 代回去就可以了：

$$
\begin{aligned}
\mathbb{P}(X - \mathbb{E}(X)\ge t)&\le \frac{\mathrm{Var}(X) + a^2}{\left(t + a\right)^2}\\
&=\frac{\mathrm{Var}(X) + \left(\frac{\mathrm{Var}(X)}{t}\right)^2}{\left(t + \frac{\mathrm{Var}(X)}{t}\right)^2}\\
&=\frac{\mathrm{Var}(X)\cdot t^2+\left(\mathrm{Var}(X)\right)^2}{\left(t^2 + \mathrm{Var}(X)\right)^2}\\
&=\frac{\mathrm{Var}(X)\cdot\left(\mathrm{Var}(X) + t^2\right)}{\left(\mathrm{Var}(X) + t^2\right)^2}\\
&=\frac{\mathrm{Var}(X)}{\mathrm{Var}(X) + t^2}
\end{aligned}
$$

于是乎我们就得到了一个比较紧的不等式。

{% enddetails %}

弱大数定理的描述很简单，其实就是说当样本足够大的时候，sample mean 会收敛到 population mean。更数学一点，就是对于任意的 $$\epsilon>0$$，我们都有：

$$
\mathbb{P}\left(\left|\overline{X}_n-\mu\right|>\epsilon\right)\to 0 \text{ as } n\to\infty
$$

证明的话用切比雪夫搞搞就可以了。

强大数定理：

$$
\mathbb{P}\left(\lim_{n\to\infty}\overline{X}_n=\mu\right)=1
$$

然后是 Central Limit Theorem (CLT)。我们令：

$$
S_n=\sum_{i=1}^nX_i
$$

那么：

$$
\lim_{n\to\infty}\mathbb{P}\left(\frac{S_n}{\sigma\sqrt{n}}\le x\right)=\Phi(x)
$$

{% details 证明 %}

上课的时候是只证了 MGF 存在的情况，看看暑假的时候有没有空写一写用 characteristic functions 的方法。

就我们假设 $$Z_n=\frac{S_n}{\sigma\sqrt{n}}$$，$$\frac{X_i-\mu}{\sigma}$$ 的 MGF 是 $$M(t)$$。

我们考虑：

$$
M_{\frac{S_n-n\mu}{\sigma}}(t)=\left(M(t)\right)^n
$$

于是我们考虑 $$Z_n$$ 的 MGF：

$$
\begin{aligned}
M_{Z_n}(t)&=\mathbb{E}\left[e^{tZ_n}\right]\\
&=\mathbb{E}\left[e^{t\frac{S_n-n\mu}{\sigma\sqrt{n}}}\right]\\
&=\mathbb{E}\left[e^{\frac{t}{\sqrt{n}}\cdot\frac{\left(S_n-\mu\right)}{\sigma}}\right]\\
&=M_{\frac{S_n-n\mu}{\sigma}}\left(\frac{t}{\sqrt{n}}\right)\\
&=\left(M\left(\frac{t}{\sqrt{n}}\right)\right)^n
\end{aligned}
$$

然后我们考虑 $$M(t)$$ 的泰勒展开：

$$
M(t)=M(0)+M'(0)t+\frac{M''(0)}{2!}t^2+\varepsilon(t)
$$

其中：

$$
\lim_{t\to 0}\frac{\varepsilon(t)}{t^2}=0
$$

然后我们考虑到：

$$
\begin{cases}
\mathbb{E}\left[\frac{X-\mu}{\sigma}\right]=0\\
\mathrm{Var}\left[\frac{X-\mu}{\sigma}\right]=1
\end{cases}
$$

于是我们有：

$$
\begin{cases}
M(0)=1\\
M'(0)=\mathbb{E}\left[\frac{X-\mu}{\sigma}\right]=0\\
M''(0)=\mathbb{E}\left[\left(\frac{X-\mu}{\sigma}\right)^2\right]=\mathrm{Var}\left[\frac{X-\mu}{\sigma}\right]=1
\end{cases}
$$

然后代进去！

$$
\begin{aligned}
M_{Z_n}(t)&=\left(M\left(\frac{t}{\sqrt{n}}\right)\right)^n\\
&=\left(M(0) + M'(0)\cdot t + M''(0)\cdot \frac{\left(\frac{t}{\sqrt{n}}\right)^2}{2}+\varepsilon\left(\frac{t}{\sqrt{n}}\right)\right)^n\\
&=\left(1 + \frac{t^2}{2n} + \mathcal{O}\left(\frac{t^2}{n}\right)\right)^n
\end{aligned}
$$

又因为我们知道：

$$
\lim_{n\to\infty}\left(1 + \frac{t}{2n}\right)^n=e^{t}
$$

于是乎我们有：

$$
\lim_{n\to\infty}M_{Z_n}(t)=e^{\frac{t^2}{2}}
$$

而 $$e^{\frac{t^2}{2}}$$ 就是 $$\mathcal{N}(0, 1)$$ 的 MGF，于是乎我们成功证明了 CLT。

{% enddetails %}

---

## Parameter Estimation

有两种主要的 type of statistics，一种是 point statistics，一种是 interval statistics。顾名思义就是一种是直接去研究点的比如 sample mean，另一种是去研究区间比如 confidence interval。

### Point Estimation

这边主要先讲的是 point statistics。

现在假设我们有数据 $$\boldsymbol{x}=(x_1, x_2, \cdots, x_n)$$，我们认为他是由随机变量 $$\boldsymbol{X}=(X_1, X_2, \cdots, X_n)$$ 所生成的，

然后我们假设知道了 $$X$$ 的分布应该是由某个参数 $$\boldsymbol{\theta}$$ 决定，然后我们 point estimation 就是找到一个合适的函数 $$\hat{\boldsymbol{\theta}}(\boldsymbol{X})$$ 来估计 $$\boldsymbol{\theta}$$。

我们考虑如何评估一个 $$\hat{\theta}$$ 的好坏，我们提出三个量：$$\mathrm{Bias}(\hat{\theta})$$, $$\mathrm{SE}(\hat{\theta})$$ 和 $$\mathrm{MSE}(\hat{\theta})$$。

其中 Bias 预估的是 $$\hat{\theta}$$ 对不对：

$$
\mathrm{Bias}(\hat{\theta})=\mathbb{E}(\hat{\theta})-\theta
$$

如果 $$\mathrm{Bias}(\hat{\theta})=0$$，那么我们就说 $$\hat{\theta}$$ 是 unbiased 的。

Standard Error 来评估的是 $$\hat{\theta}$$ 的稳定性：

$$
\mathrm{SE}(\hat{\theta})=\sqrt{\mathrm{Var}(\hat{\theta})}
$$

而最后 Mean Squared Error 是综合评定了 $$\hat{\theta}$$ 的准不准：

$$
\mathrm{MSE}(\hat{\theta})=\mathbb{E}\left[\left(\hat{\theta}-\theta\right)^2\right]
$$

而其中我们有：

$$
\mathrm{MSE}(\hat{\theta})=\mathrm{Bias}(\hat{\theta})^2+\mathrm{SE}(\hat{\theta})^2
$$

{% details 证明 %}

$$
\begin{aligned}
\mathrm{MSE}(\hat{\theta})&=\mathbb{E}\left(\left(\hat{\theta}-\theta\right)^2\right)\\
&=\mathbb{E}\left(\hat{\theta}^2-2\cdot\hat{\theta}\cdot\theta+\theta^2\right)\\
&=\mathbb{E}\left(\hat{\theta}^2\right)-2\cdot\theta\cdot\mathbb{E}\left(\hat{\theta}\right)+\theta^2\\
&=\left[\mathbb{E}\left(\hat{\theta}^2\right)-\left(\mathbb{E}(\hat{\theta})\right)^2\right]+\left[\left(\mathbb{E}(\hat{\theta})\right)^2-2\cdot\theta\cdot\mathbb{E}\left(\hat{\theta}\right)+\theta^2\right]\\
&=\mathrm{Var}(\hat{\theta})+\left[\mathbb{E}(\hat{\theta})-\theta\right]^2\\
&=\mathrm{SE}(\hat{\theta})^2+\mathrm{Bias}(\hat{\theta})^2
\end{aligned}
$$

不是很理解为什么如果不在长公式后随便加一句话 presentation 就会出问题啊啊啊啊啊啊啊啊啊啊

{% enddetails %}
